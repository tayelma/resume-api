The Cloud Resume API Challenge 

---
Objectives:

The objective of the project is to convert resume data into json format stored in a database to be fetched over an api call to display the json data. 
The json data is stored in a Dynamodb table and a lambda function retrieves the data from the table and displays the data via the api endpoint generated by API Gateway anytime there is an api call.

---
Resume data conversion to json:

I used this link https://jsonresume.org/schema to generate my resume json data for the project.

---

AWS Lambda Function Explanation:

This Python code is an AWS Lambda function designed to fetch a specific user's data from a DynamoDB table. Here's a detailed breakdown of its purpose, functionality, and implementation.

---

 Key Features
1. Fetches User Data: Retrieves a record from DynamoDB using the hardcoded user ID `thelma_laryea`.
2. Environment Configuration: The DynamoDB table name and AWS region are dynamically set using environment variables.
3. Error Handling: Includes robust error handling for logging and responding to potential issues during execution.
4. Logging: Provides detailed logs for tracking requests and identifying problems.

---

 Step-by-Step Explanation

 1. Importing Required Libraries
- `json`: Used to handle JSON data, particularly for responses.
- `boto3`: AWS SDK for Python, used to interact with DynamoDB.
- `os`: Retrieves environment variables like `AWS_REGION` and `DYNAMODB_TABLE`.
- `logging`: Logs events for monitoring and debugging.
- `ClientError`: Captures errors that might occur during DynamoDB operations.

 2. Logging Configuration

logger = logging.getLogger()
logger.setLevel(logging.INFO)

- Configures logging for the Lambda function.
- Sets the log level to `INFO` to capture detailed execution information.

---

 3. Initializing DynamoDB Client

dynamodb = boto3.client('dynamodb', region_name=os.environ.get('AWS_REGION', 'us-east-1'))

- Creates a DynamoDB client using the AWS SDK.
- Retrieves the AWS region from the environment variable `AWS_REGION`. 
- Optimization: The client is initialized outside the handler function for reusability across multiple invocations, improving performance.

---

 4. Handler Function
The `lambda_handler` function is the entry point for AWS Lambda.

 Parameters:
- `event`: Contains the request payload and input data.
- `context`: Provides runtime information, such as the AWS request ID and function name.

 Steps Inside the Handler:

---

 5. Defining Key Variables

table_name = os.environ.get('DYNAMODB_TABLE', 'ResumeTable-dev')
user_id = 'thelma_laryea'

- `table_name`: Retrieves the DynamoDB table name from the environment variable `DYNAMODB_TABLE`. Defaults to `ResumeTable-dev`.
- `user_id`: The hardcoded user ID whose data is being fetched (`thelma_laryea`).

---

 6. Logging Context Information

request_id = context.aws_request_id
function_name = context.function_name
logger.info(f"Request ID: {request_id}, Function: {function_name}, Fetching data for user_id: {user_id}")

- Logs important context details:
  - Request ID: Unique identifier for the current invocation.
  - Function Name: Name of the Lambda function.
  - User ID: Identifies the record being fetched.

---

 7. Fetching Data from DynamoDB

response = dynamodb.get_item(
    TableName=table_name,
    Key={
        'user_id': {'S': user_id}
    }
)

- `get_item`: Retrieves a single item from the DynamoDB table.
- `Key`: Specifies the primary key (`user_id`) to locate the record.
- `'S'`: Denotes that the `user_id` is a string type in DynamoDB.

---

 8. Processing the Response

item = response.get('Item')
if item:
    logger.info(f"Successfully fetched item for user_id: {user_id}")
    return {
        'statusCode': 200,
        'body': json.dumps(item),
        ...
    }
else:
    logger.warning(f"Item not found for user_id: {user_id}")
    return {
        'statusCode': 404,
        'body': json.dumps({'message': 'Item not found'}),
        ...
    }

- Checks if the item was found:
  - Found: Logs success and returns the data with a `200 OK` status code.
  - Not Found: Logs a warning and returns a `404 Not Found` status code with a descriptive message.

---

 9. Error Handling

except ClientError as e:
    logger.error(f"Error fetching data from DynamoDB for user_id: {user_id}. Exception: {e}", exc_info=True)
    return {
        'statusCode': 500,
        'body': json.dumps({'message': 'Error fetching data from DynamoDB', 'error': str(e)}),
        ...
    }

- Catches DynamoDB Errors: Logs the error with detailed traceback for debugging.
- Returns a `500 Internal Server Error` status code with an error message and details about the exception.

---

 10. Returning the Response
Each response includes:
- `statusCode`: HTTP status code indicating success, not found, or server error.
- `body`: JSON-encoded response data or error message.
- `headers`:
  - `Content-Type`: Specifies the response format as JSON.
  - `Access-Control-Allow-Origin`: Enables cross-origin resource sharing (CORS).

---

Infrastructure as Code (IaC):

Terraform was used to create the aws infrastructure to deploy the project, the IaC files were configured into modules with each AWS resource configured in a module:

Provider.tf

This file sets up Terraform and the AWS provider for managing infrastructure:

---

- Terraform Version: Ensures version 1.9.3 or higher is used.
- AWS Provider: Uses the `aws` provider, pinned to version 5.60.x.
- Backend (`s3`):
  - Stores Terraform's state file in the S3 bucket `states-tf-projects` under the path `cloud-resume-api/terraform.tfstate`.
  - Uses DynamoDB (`terraform` table) for state locking, preventing simultaneous updates.

- Profile: Authenticates using the `Hilary` AWS CLI profile.
- Region: Dynamically uses the value of the `var.region` variable for flexibility.


Variables.tf:

This Terraform file is self-explanatory and is purposely to declare variables to ensure easy adjustment of configurations, reusability (consistent names) and scalability. 

s3.tf:

This Terraform configuration sets up an S3 bucket, zips a Lambda function code directory, and uploads the zipped file to the S3 bucket. 


output.tf:

The output block is used in Terraform to display useful information (the API Gateway's endpoint URL as shown below) after the infrastructure is deployed.
API Gateway endpoint: https://o0zc7ausgc.execute-api.us-east-1.amazonaws.com/dev/
NB: attach the route key "/resume" to the above endpoint to display the resume json data in the browser. Hence the accessible link is: 
https://o0zc7ausgc.execute-api.us-east-1.amazonaws.com/dev/resume


lambda.tf:

This Terraform configuration defines an AWS Lambda function named resume_api_function. It deploys a Python-based Lambda function using a zipped code file stored in an S3 bucket.

iam.tf:

This Terraform code defines the permissions and role required for the Lambda function to interact with AWS services securely and effectively.

dynamodb.tf

This Terraform code creates a DynamoDB table and inserts an item into it.


api gateway.tf

This Terraform code defines resources for setting up an API Gateway and integrates it with a Lambda function.


Workflow Summary
API Gateway Setup:

An API Gateway is created with a name that includes the environment (e.g., resume-api-dev).
A stage (dev) is set up for deployment, with auto-deployment enabled.
Lambda Integration:

The POST method for the /resume route is linked to the Lambda function using the AWS_PROXY integration type.
When the API receives a GET /resume request, it will invoke the Lambda function.
Lambda Permissions:

Permissions are granted to API Gateway, allowing it to invoke the Lambda function.

---
AUTOMATION:


GitHub Actions

GitHub Actions Workflow: Deploy Lambda Function

This workflow. is designed to automate the process of packaging and deploying an AWS Lambda function whenever changes are merged into the `main` branch through a pull request. It consists of two main stages: packaging the Lambda function's code and deploying it to AWS.

---

 Trigger Events
The workflow runs when:
1. A pull request targeting the `main` branch is closed.
2. The pull request is merged into the `main` branch. 

These conditions ensure that the deployment only happens after the code is reviewed and approved.

---

 Environment Variables
The workflow uses the following environment variables:
- `S3_BUCKET_NAME`: The name of the S3 bucket where the Lambda deployment package will be stored (e.g., `resume-api-lambda`).
- `LAMBDA`: The name of the AWS Lambda function being deployed (e.g., `resume_function`).

These variables simplify the configuration process by allowing changes in bucket names or function names without modifying the entire workflow.

---

 Jobs in the Workflow

 1. `package` Job
This job prepares the Lambda function's code for deployment by performing the following tasks:

1. Check if the Pull Request is Merged  
   The job only runs if the pull request is merged. This condition ensures that unapproved changes don’t trigger unnecessary actions.

2. Steps in the `package` Job:
   - Checkout Repository:  
     The repository code is cloned into the runner using the `actions/checkout` action. This makes the Lambda function's code available for packaging.
     
   - Generate Version:  
     The workflow uses a semantic versioning tool (`paulhatch/semantic-version`) to generate a version number for the Lambda package.  
     - Versions follow a format like `1.0.0-release1` and are based on commit messages in the repository.
     - This ensures that every deployment package has a unique version, making it easier to track and roll back if necessary.
     
   - Configure AWS Credentials:  
     AWS credentials are set up using secrets stored in GitHub. These credentials allow the workflow to interact securely with AWS services.

   - Create a ZIP File for the Lambda Code:  
     The Lambda function code (`api/resume-lambda-code.py`) is compressed into a ZIP file.  
     - The ZIP file name is generated dynamically using the Lambda function name and the semantic version, e.g., `resume_function-1.0.0-release1.zip`.

   - Upload the ZIP File to S3:  
     The packaged ZIP file is uploaded to the specified S3 bucket using the AWS CLI. The file path in S3 will look like:  
     s3://resume-api-lambda/resume_function-1.0.0-release1.zip
     

3. Output:  
   The `package` job exports the generated version number as an output, which is used in the next job.

---

 2. `deploy` Job
This job deploys the packaged Lambda code to AWS.

1. Conditions:  
   The job runs only if the GitHub Actions runner is working on the `main` branch.

2. Dependencies:  
   It depends on the successful completion of the `package` job. If the packaging fails, deployment won’t occur.

3. Steps in the `deploy` Job:
   - Configure AWS Credentials:  
     Like the `package` job, AWS credentials are configured to allow secure interaction with AWS.

   - Update the Lambda Function:  
     The workflow uses the AWS CLI to update the Lambda function's code.  
     - It specifies the S3 bucket and file path (using the version generated in the `package` job) as the source for the deployment.
     - The command ensures the Lambda function uses the latest code uploaded to S3.

---

 How the Workflow Works
1. Code Change Submission:  
   Changes are submitted via a pull request to the `main` branch.

2. Pull Request Merged:  
   Once the pull request is reviewed, approved, and merged, the workflow is triggered.

3. Packaging:  
   The `package` job:
   - Clones the repository.
   - Generates a version number.
   - Creates a ZIP package of the Lambda code.
   - Uploads the package to S3.

4. Deployment:  
   The `deploy` job:
   - Updates the Lambda function to use the latest package stored in S3.

---

 Benefits of This Workflow
- Automation: Removes the need for manual packaging and deployment, saving time and reducing errors.
- Versioning: Each deployment package is tagged with a unique version for easy tracking.
- Safety: Ensures that only reviewed and approved code is deployed.
- Scalability: Can be easily extended to handle multiple Lambda functions or other deployment scenarios.

---

 Example Output
After the workflow runs:
- The Lambda function `resume_function` is updated with the latest code.
- The deployment package (e.g., `resume_function-1.0.0-release1.zip`) is stored in the S3 bucket `resume-api-lambda`.

---

Challenges and future improvements
Faced some errors fetching data from the user id 'thelma_laryea'. For improvements, the code will be enhanced for scalability if the user id isn't hard coded.


