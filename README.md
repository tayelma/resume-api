The Cloud Resume API Challenge 

---
Objectives:

The objective of the project is to convert resume data into json format stored in a database to be fetched over an api call to display the json data. 
The json data is stored in a Dynamodb table and a lambda function retrieves the data from the table and displays the data via the api endpoint generated by API Gateway anytime there is an api call.

---
Resume data conversion to json:

I used this link https://jsonresume.org/schema to generate my resume json data for the project.

---

AWS Lambda Function Explanation:

This Python code is an AWS Lambda function designed to fetch a specific user's data from a DynamoDB table. Here's a detailed breakdown of its purpose, functionality, and implementation.

---

 Key Features
1. Fetches User Data: Retrieves a record from DynamoDB using the hardcoded user ID `thelma_laryea`.
2. Environment Configuration: The DynamoDB table name and AWS region are dynamically set using environment variables.
3. Error Handling: Includes robust error handling for logging and responding to potential issues during execution.
4. Logging: Provides detailed logs for tracking requests and identifying problems.

---

 Step-by-Step Explanation

 1. Importing Required Libraries
- `json`: Used to handle JSON data, particularly for responses.
- `boto3`: AWS SDK for Python, used to interact with DynamoDB.
- `os`: Retrieves environment variables like `AWS_REGION` and `DYNAMODB_TABLE`.
- `logging`: Logs events for monitoring and debugging.
- `ClientError`: Captures errors that might occur during DynamoDB operations.

 2. Logging Configuration

logger = logging.getLogger()
logger.setLevel(logging.INFO)

- Configures logging for the Lambda function.
- Sets the log level to `INFO` to capture detailed execution information.

---

 3. Initializing DynamoDB Client

dynamodb = boto3.client('dynamodb', region_name=os.environ.get('AWS_REGION', 'us-east-1'))

- Creates a DynamoDB client using the AWS SDK.
- Retrieves the AWS region from the environment variable `AWS_REGION`. 
- Optimization: The client is initialized outside the handler function for reusability across multiple invocations, improving performance.

---

 4. Handler Function
The `lambda_handler` function is the entry point for AWS Lambda.

 Parameters:
- `event`: Contains the request payload and input data.
- `context`: Provides runtime information, such as the AWS request ID and function name.

 Steps Inside the Handler:

---

 5. Defining Key Variables

table_name = os.environ.get('DYNAMODB_TABLE', 'ResumeTable-dev')
user_id = 'thelma_laryea'

- `table_name`: Retrieves the DynamoDB table name from the environment variable `DYNAMODB_TABLE`. Defaults to `ResumeTable-dev`.
- `user_id`: The hardcoded user ID whose data is being fetched (`thelma_laryea`).

---

 6. Logging Context Information

request_id = context.aws_request_id
function_name = context.function_name
logger.info(f"Request ID: {request_id}, Function: {function_name}, Fetching data for user_id: {user_id}")

- Logs important context details:
  - Request ID: Unique identifier for the current invocation.
  - Function Name: Name of the Lambda function.
  - User ID: Identifies the record being fetched.

---

 7. Fetching Data from DynamoDB

response = dynamodb.get_item(
    TableName=table_name,
    Key={
        'user_id': {'S': user_id}
    }
)

- `get_item`: Retrieves a single item from the DynamoDB table.
- `Key`: Specifies the primary key (`user_id`) to locate the record.
- `'S'`: Denotes that the `user_id` is a string type in DynamoDB.

---

 8. Processing the Response

item = response.get('Item')
if item:
    logger.info(f"Successfully fetched item for user_id: {user_id}")
    return {
        'statusCode': 200,
        'body': json.dumps(item),
        ...
    }
else:
    logger.warning(f"Item not found for user_id: {user_id}")
    return {
        'statusCode': 404,
        'body': json.dumps({'message': 'Item not found'}),
        ...
    }

- Checks if the item was found:
  - Found: Logs success and returns the data with a `200 OK` status code.
  - Not Found: Logs a warning and returns a `404 Not Found` status code with a descriptive message.

---

 9. Error Handling

except ClientError as e:
    logger.error(f"Error fetching data from DynamoDB for user_id: {user_id}. Exception: {e}", exc_info=True)
    return {
        'statusCode': 500,
        'body': json.dumps({'message': 'Error fetching data from DynamoDB', 'error': str(e)}),
        ...
    }

- Catches DynamoDB Errors: Logs the error with detailed traceback for debugging.
- Returns a `500 Internal Server Error` status code with an error message and details about the exception.

---

 10. Returning the Response
Each response includes:
- `statusCode`: HTTP status code indicating success, not found, or server error.
- `body`: JSON-encoded response data or error message.
- `headers`:
  - `Content-Type`: Specifies the response format as JSON.
  - `Access-Control-Allow-Origin`: Enables cross-origin resource sharing (CORS).

---

Infrastructure as Code (IaC):

Terraform was used to create the aws infrastructure to deploy the project, the IaC files were configured into modules with each AWS resource configured in a module:

Provider.tf

This file sets up Terraform and the AWS provider for managing infrastructure:

---

- Terraform Version: Ensures version 1.9.3 or higher is used.
- AWS Provider: Uses the `aws` provider, pinned to version 5.60.x.
- Backend (`s3`):
  - Stores Terraform's state file in the S3 bucket `states-tf-projects` under the path `cloud-resume-api/terraform.tfstate`.
  - Uses DynamoDB (`terraform` table) for state locking, preventing simultaneous updates.

- Profile: Authenticates using the `Hilary` AWS CLI profile.
- Region: Dynamically uses the value of the `var.region` variable for flexibility.


Variables.tf:

This Terraform file is self-explanatory and is purposely to declare variables to ensure easy adjustment of configurations, reusability (consistent names) and scalability. 

s3.tf:

This Terraform configuration sets up an S3 bucket, zips a Lambda function code directory, and uploads the zipped file to the S3 bucket. 


output.tf:

The output block is used in Terraform to display useful information (the API Gateway's endpoint URL as shown below) after the infrastructure is deployed.
API Gateway endpoint: https://o0zc7ausgc.execute-api.us-east-1.amazonaws.com/dev/
NB: attach the route key "/resume" to the above endpoint to display the resume json data in the browser. Hence the accessible link is: 
https://o0zc7ausgc.execute-api.us-east-1.amazonaws.com/dev/resume


lambda.tf:

This Terraform configuration defines an AWS Lambda function named resume_api_function. It deploys a Python-based Lambda function using a zipped code file stored in an S3 bucket.

iam.tf:

This Terraform code defines the permissions and role required for the Lambda function to interact with AWS services securely and effectively.

dynamodb.tf

This Terraform code creates a DynamoDB table and inserts an item into it.


api gateway.tf

This Terraform code defines resources for setting up an API Gateway and integrates it with a Lambda function.


Workflow Summary
API Gateway Setup:

An API Gateway is created with a name that includes the environment (e.g., resume-api-dev).
A stage (dev) is set up for deployment, with auto-deployment enabled.
Lambda Integration:

The POST method for the /resume route is linked to the Lambda function using the AWS_PROXY integration type.
When the API receives a GET /resume request, it will invoke the Lambda function.
Lambda Permissions:

Permissions are granted to API Gateway, allowing it to invoke the Lambda function.

---
AUTOMATION:


GitHub Actions

GitHub Actions Workflow: Deploy Lambda Function

This workflow. is designed to automate the process of packaging and deploying an AWS Lambda function whenever changes are merged into the `main` branch through a pull request. It consists of two main stages: packaging the Lambda function's code and deploying it to AWS.

---

 Trigger Events
The workflow runs when:
1. A pull request targeting the `main` branch is closed.
2. The pull request is merged into the `main` branch. 

These conditions ensure that the deployment only happens after the code is reviewed and approved.

---

 Environment Variables
The workflow uses the following environment variables:
- `S3_BUCKET_NAME`: The name of the S3 bucket where the Lambda deployment package will be stored (e.g., `resume-api-lambda`).
- `LAMBDA`: The name of the AWS Lambda function being deployed (e.g., `resume_function`).

These variables simplify the configuration process by allowing changes in bucket names or function names without modifying the entire workflow.

---

 Jobs in the Workflow

 1. `package` Job
This job prepares the Lambda function's code for deployment by performing the following tasks:

1. Check if the Pull Request is Merged  
   The job only runs if the pull request is merged. This condition ensures that unapproved changes donâ€™t trigger unnecessary actions.

2. Steps in the `package` Job:
   - Checkout Repository:  
     The repository code is cloned into the runner using the `actions/checkout` action. This makes the Lambda function's code available for packaging.
     
   - Generate Version:  
     The workflow uses a semantic versioning tool (`paulhatch/semantic-version`) to generate a version number for the Lambda package.  
     - Versions follow a format like `1.0.0-release1` and are based on commit messages in the repository.
     - This ensures that every deployment package has a unique version, making it easier to track and roll back if necessary.
     
   - Configure AWS Credentials:  
     AWS credentials are set up using secrets stored in GitHub. These credentials allow the workflow to interact securely with AWS services.

   - Create a ZIP File for the Lambda Code:  
     The Lambda function code (`api/resume-lambda-code.py`) is compressed into a ZIP file.  
     - The ZIP file name is generated dynamically using the Lambda function name and the semantic version, e.g., `resume_function-1.0.0-release1.zip`.

   - Upload the ZIP File to S3:  
     The packaged ZIP file is uploaded to the specified S3 bucket using the AWS CLI. The file path in S3 will look like:  
     s3://resume-api-lambda/resume_function-1.0.0-release1.zip
     

3. Output:  
   The `package` job exports the generated version number as an output, which is used in the next job.

---

 2. `deploy` Job
This job deploys the packaged Lambda code to AWS.

1. Conditions:  
   The job runs only if the GitHub Actions runner is working on the `main` branch.

2. Dependencies:  
   It depends on the successful completion of the `package` job. If the packaging fails, deployment wonâ€™t occur.

3. Steps in the `deploy` Job:
   - Configure AWS Credentials:  
     Like the `package` job, AWS credentials are configured to allow secure interaction with AWS.

   - Update the Lambda Function:  
     The workflow uses the AWS CLI to update the Lambda function's code.  
     - It specifies the S3 bucket and file path (using the version generated in the `package` job) as the source for the deployment.
     - The command ensures the Lambda function uses the latest code uploaded to S3.

---

 How the Workflow Works
1. Code Change Submission:  
   Changes are submitted via a pull request to the `main` branch.

2. Pull Request Merged:  
   Once the pull request is reviewed, approved, and merged, the workflow is triggered.

3. Packaging:  
   The `package` job:
   - Clones the repository.
   - Generates a version number.
   - Creates a ZIP package of the Lambda code.
   - Uploads the package to S3.

4. Deployment:  
   The `deploy` job:
   - Updates the Lambda function to use the latest package stored in S3.

---

 Benefits of This Workflow
- Automation: Removes the need for manual packaging and deployment, saving time and reducing errors.
- Versioning: Each deployment package is tagged with a unique version for easy tracking.
- Safety: Ensures that only reviewed and approved code is deployed.
- Scalability: Can be easily extended to handle multiple Lambda functions or other deployment scenarios.

---

 Example Output
After the workflow runs:
- The Lambda function `resume_function` is updated with the latest code.
- The deployment package (e.g., `resume_function-1.0.0-release1.zip`) is stored in the S3 bucket `resume-api-lambda`.

---

Challenges and future improvements
Faced some errors fetching data from the user id 'thelma_laryea'. For improvements, the code will be enhanced for scalability if the user id isn't hard coded.


